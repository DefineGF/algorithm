##### 构造神经网络 - 继承 torch.nn.Module

```python
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net,self).__init__() # 继承 __init__ 特性
        self.hidden = torch.nn.Linear(n_feature, n_hidden) # 计算隐藏层
        self.predict = torch.nn.Linear(n_hidden, n_output) # 计算输出层

    def forward(self, x):              # 前向传播 
        x = torch.relu(self.hidden(x)) # 隐藏层输入值
        x = self.predict(x)            # 隐藏层输出值
        return x
    
    
# 构造神经网路
net = Net(n_feature=1, n_hidden=10, n_output=1) 
print(net) # 可以输出结构
```



##### 生成数据集

```python
def data_generator():
    x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)
    y = x.pow(2) + 0.2 * torch.rand(x.size())
    return x, y

x, y = data_generator()
```

##### 训练

```python
# SGD 随机梯度下降算法
optimizer = torch.optim.SGD(net.parameters(), lr = 0.2) # 传入net所有参数 & 学习率
loss_func = torch.nn.MSELoss() # 均方差 --> 误差计算公式

# 迭代更新参数
for i in range(100):
    prediction = net(x)
    loss = loss_func(prediction, y) # 传入预测值 & 实际值
    optimizer.zero_grad()           # 清空上一步的残余梯度值
    loss.backward()                 # 误差反向传播，计算参数更新值
    optimizer.step()                # 将更新的参数值施加到 net 的 parameters
    # 可视化过程
    if i % 5 == 0:
        plt.cla()
        plt.plot(x.data.numpy(), prediction.data.numpy(), lw=3, color='red')
        plt.scatter(x.data.numpy(), y.data.numpy())
        plt.text(0.5, 0, 'loss=%.4f' % loss.data.numpy(), 
                 fontdict={'size':20,'color':'red'})
        plt.pause(0.1)
        plt.show()
```





##### 全连接神经网络训练流程

1. 创建神经网络

    - 重写 _ _ init_ _() 方法，传入隐藏层、输出层参数；
    - 重写 forward() 方法，添加激活函数，并返回预测值；

2. 设置随机下降梯度函数，传入神经网络参数和学习率；

    ```python
    optimizer = torch.optim.SGD(net.parameters(), lr = 0.2) # 传入net所有参数 & 学习率
    ```

3. 设置损失函数

    ```python
    loss_func = torch.nn.MSELoss() # 均方差 --> 误差计算公式
    ```

4. 迭代修改参数

    1. 通过net(intput) 计算预测值

        ```python
        prediction = net(x)
        ```

    2. 通过损失函数计算损失值：

        ```python
         loss = loss_func(prediction, y)
        ```

    3. 清空残余梯度值：

        ```python
        optimizer.zero_grad() # 清空上一步的残余梯度值
        ```

    4. 反向传播获取新的梯度值：

        ```python
        loss.backward() # 误差反向传播，计算参数更新值
        
        ```

    5. 更新神经网络参数

        ```python
        optimizer.step() # 将更新的参数值施加到 net 的 parameters
        
        ```

       
